<p align="left"> </p>

<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT">
<a href="https://standardjs.com"><img src="https://img.shields.io/badge/code_style-standard-brightgreen.svg" alt="Standard - \Matlab Style Guide"></a>

# Fairness vs. Distributional Shift
## Table Contents
* [Project Description](#PD)
* [Objectives](OJ)
* [Collaborators](CO)
* [Aknowledgement](AK)
* [License](LI)
<a id = "PD"></a>
## Project Description
Ensuring fairness mechanisms have been proposed to quantitatively expose veiling unfairness in sensitive attributes. For example, demographic parity, equal opportunity, and equalized odds for group fairness and fairness through awareness and counterfactual fairness for individual fairness. When verifying fairness using these aforementioned metrics, most of the sensitive attributes are premised on categorial values, such as gender, ethnicity, and marital status, rather than numerical variables. Consequently, skin color is categorized into skin type, and age is converted to generation. However, this makes fairness assurance degraded. To solve this challenge, we propose our new measure to identify the correlation by statistical distance metrics without categorization. I verify it in the skin color and skin lesion classification setting.

<a id = "OJ"></a>
## Objectives
* Indetify the skin color distribution correlate with fairness metrics in skin lesion classification.
* Identify the correlation affect diagnostic accuracy and reliability across diverse skin color.
* Compare the fairness assurance degradation to skin color types and skin color distribution.

<a id = "CO"></a>
## Collaborators
Kuniko Paxton<br>
Dr Koorosh Aslansefat

<a id = "AK"></a>
## Aknowledgement
We would like to thank Responsible AI Hull Research Group and DAIM at University of Hull for their support.

<a id = "LI"></a>
## License
This framework is available under an MIT License.

