<p align="left"> </p>

<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT">
<a href="https://standardjs.com"><img src="https://img.shields.io/badge/code_style-standard-brightgreen.svg" alt="Standard - \Matlab Style Guide"></a>

# Fairness vs. Distributional Shift
## Table Contents
* [Project Description](#PD)
* [Objectives](OJ)
* [Collaborators](CO)
* [Aknowledgement](CO)
* [Reference](RF)
* [License]
<a id = "PD"></a>
## Project Description
Ensuring fairness mechanisms have been proposed to quantitatively expose veiling unfairness in sensitive attributes. For example, demographic parity, equal opportunity, and equalized odds for group fairness and fairness through awareness and counterfactual fairness for individual fairness. When verifying fairness using these aforementioned metrics, most of the sensitive attributes are premised on categorial values, such as gender, ethnicity, and marital status rather than numerical variables. Consequently, skin color categorized into skin type and age converted to generation. 

<a id = "OJ"></a>
## Objectives

## Aknowledgement
We would like to thank Responsible AI Hull Research Group and DAIM at University of Hull for their support.

## License
This framework is available under an MIT License.

